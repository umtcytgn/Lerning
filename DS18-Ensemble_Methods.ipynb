{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Ensemble Methods\n",
    "---\n",
    "\n",
    "_Author: Carleton L. Smith_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<a id = \"top\"></a>\n",
    "## Questions\n",
    "- [Question 1](#q1)\n",
    "- [Question 2](#q2)\n",
    "- [Question 3](#q3)\n",
    "- [Question 4](#q4)\n",
    "- [Question 5](#q5)\n",
    "- [Question 6](#q6)\n",
    "- [Question 7](#q7)\n",
    "- [Question 8](#q8)\n",
    "- [Question 9](#q9)\n",
    "- [Question 10](#q10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Assignment Overview\n",
    "\n",
    "This assignment will:\n",
    "\n",
    "- Discuss ensemble learning\n",
    "- Introduce two powerful ensemble methodologies: Bagging and Boosting\n",
    "- Discuss the mechanics of ensemble algorithms: Random Forest, AdaBoost, and Gradient Boosted Trees (GBT)\n",
    "- Test your ability to create a Random Forest, AdaBoost, and GBT model in `scikit-learn`\n",
    "\n",
    "**EXPECTED TIME: 2 HRS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Assignment Contents\n",
    "\n",
    " - [Introduction](#intro)\n",
    "     - [Use Case and Dataset](#data)\n",
    "     - [Quick Exploration](#explore)\n",
    " - [Ensemble Learning](#ensembles)\n",
    " - [Bagging: _Boostrap Aggregation_](#bagging)\n",
    "     - [Why Bagging Works](#bagging-works)\n",
    "     - [Bootstrap Sampling](#bootstrap)\n",
    "     - [Classification Metrics Review](#classification-metrics)\n",
    "     - [Random Forest](#random-forest)\n",
    "     - [ExtraTreesClassifier](#extra-trees)\n",
    "     - [Out-of-Bag Evaluation](#oob)\n",
    " - [Boosting](#boosting)\n",
    "     - [AdaBoost](#adaboost)\n",
    "     - [Gradient Boosted Trees (GBT)](#gbt)\n",
    " - [Feature Importance](#feature-importance)\n",
    " - [References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<a id = \"intro\"></a>\n",
    "## Introduction\n",
    "\n",
    "Ensemble methods are popular machine learning procedures because of their performance and interpretability. This assignment will test your understanding of ensembles and your ability to create, fine tune, and interpret their output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# constants\n",
    "FILEPATH = './assets/hmeq-preprocessed.csv'\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad_loan</th>\n",
       "      <th>loan_request</th>\n",
       "      <th>amt_due_on_mort</th>\n",
       "      <th>value_of_property</th>\n",
       "      <th>years_at_job</th>\n",
       "      <th>num_derog_reports</th>\n",
       "      <th>num_delinq_lines</th>\n",
       "      <th>oldest_cl_age</th>\n",
       "      <th>num_recent_cl</th>\n",
       "      <th>num_of_cl</th>\n",
       "      <th>debt_to_inc_ratio</th>\n",
       "      <th>reason_for_loan_DebtCon</th>\n",
       "      <th>reason_for_loan_HomeImp</th>\n",
       "      <th>occupation_Mgr</th>\n",
       "      <th>occupation_Office</th>\n",
       "      <th>occupation_Other</th>\n",
       "      <th>occupation_ProfExe</th>\n",
       "      <th>occupation_Sales</th>\n",
       "      <th>occupation_Self</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.563111</td>\n",
       "      <td>-1.092273</td>\n",
       "      <td>-1.119977</td>\n",
       "      <td>0.204896</td>\n",
       "      <td>-0.287802</td>\n",
       "      <td>-0.390777</td>\n",
       "      <td>-1.010801</td>\n",
       "      <td>-0.105982</td>\n",
       "      <td>-1.245799</td>\n",
       "      <td>0.100426</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.545527</td>\n",
       "      <td>-0.083958</td>\n",
       "      <td>-0.611919</td>\n",
       "      <td>-0.263303</td>\n",
       "      <td>-0.287802</td>\n",
       "      <td>1.444855</td>\n",
       "      <td>-0.689082</td>\n",
       "      <td>-0.693180</td>\n",
       "      <td>-0.749574</td>\n",
       "      <td>0.100426</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.527943</td>\n",
       "      <td>-1.374281</td>\n",
       "      <td>-1.506102</td>\n",
       "      <td>-0.664616</td>\n",
       "      <td>-0.287802</td>\n",
       "      <td>-0.390777</td>\n",
       "      <td>-0.365411</td>\n",
       "      <td>-0.105982</td>\n",
       "      <td>-1.146554</td>\n",
       "      <td>0.100426</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.510359</td>\n",
       "      <td>0.549123</td>\n",
       "      <td>0.142169</td>\n",
       "      <td>-0.798387</td>\n",
       "      <td>-0.287802</td>\n",
       "      <td>-0.390777</td>\n",
       "      <td>-1.022905</td>\n",
       "      <td>-0.693180</td>\n",
       "      <td>-0.749574</td>\n",
       "      <td>0.100426</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.510359</td>\n",
       "      <td>-0.985310</td>\n",
       "      <td>-1.097579</td>\n",
       "      <td>0.004239</td>\n",
       "      <td>-0.287802</td>\n",
       "      <td>-0.390777</td>\n",
       "      <td>-0.927646</td>\n",
       "      <td>-0.105982</td>\n",
       "      <td>-1.345045</td>\n",
       "      <td>0.362816</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bad_loan  loan_request  amt_due_on_mort  value_of_property  years_at_job  \\\n",
       "0       1.0     -1.563111        -1.092273          -1.119977      0.204896   \n",
       "1       1.0     -1.545527        -0.083958          -0.611919     -0.263303   \n",
       "2       1.0     -1.527943        -1.374281          -1.506102     -0.664616   \n",
       "3       0.0     -1.510359         0.549123           0.142169     -0.798387   \n",
       "4       1.0     -1.510359        -0.985310          -1.097579      0.004239   \n",
       "\n",
       "   num_derog_reports  num_delinq_lines  oldest_cl_age  num_recent_cl  \\\n",
       "0          -0.287802         -0.390777      -1.010801      -0.105982   \n",
       "1          -0.287802          1.444855      -0.689082      -0.693180   \n",
       "2          -0.287802         -0.390777      -0.365411      -0.105982   \n",
       "3          -0.287802         -0.390777      -1.022905      -0.693180   \n",
       "4          -0.287802         -0.390777      -0.927646      -0.105982   \n",
       "\n",
       "   num_of_cl  debt_to_inc_ratio  reason_for_loan_DebtCon  \\\n",
       "0  -1.245799           0.100426                        0   \n",
       "1  -0.749574           0.100426                        0   \n",
       "2  -1.146554           0.100426                        0   \n",
       "3  -0.749574           0.100426                        0   \n",
       "4  -1.345045           0.362816                        0   \n",
       "\n",
       "   reason_for_loan_HomeImp  occupation_Mgr  occupation_Office  \\\n",
       "0                        1               0                  0   \n",
       "1                        1               0                  0   \n",
       "2                        1               0                  0   \n",
       "3                        1               0                  1   \n",
       "4                        1               0                  0   \n",
       "\n",
       "   occupation_Other  occupation_ProfExe  occupation_Sales  occupation_Self  \n",
       "0                 1                   0                 0                0  \n",
       "1                 1                   0                 0                0  \n",
       "2                 1                   0                 0                0  \n",
       "3                 0                   0                 0                0  \n",
       "4                 1                   0                 0                0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read in the data and display the first 5 rows:\n",
    "hmeq = pd.read_csv(FILEPATH)\n",
    "display(hmeq.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<a id = \"data\"></a>\n",
    "### Use Case and Dataset\n",
    "\n",
    "The data for this assignment is a preprocessed version of the [HMEQ Dataset](https://www.kaggle.com/ajay1735/hmeq-data) [<sup>4</sup>](#hmeq) from Kaggle.\n",
    "\n",
    "**FEATURES:**\n",
    "\n",
    "_Note: All numeric columns are standardized._\n",
    "\n",
    "- `loan_request`:  The requested loan amount (numeric)\n",
    "- `amt_due_on_mort`: The amount due on the individual's existing mortgage (numeric)\n",
    "- `value_of_property`: The value of the current property (numeric)\n",
    "- `years_at_job`: Number of years at current job (numeric)\n",
    "- `num_derog_reports`: Number of major derogatory reports (numeric)\n",
    "- `num_delinq_lines`: Number of delinquent credit lines (numeric)\n",
    "- `oldest_cl_age`: Age of oldest trade line in standardized months (numeric)\n",
    "- `num_recent_cl`: Number of recent credit lines (numeric)\n",
    "- `num_of_cl`: Number of credit lines (numeric)\n",
    "- `debt_to_inc_ratio`: The individual's debt to income ratio (numeric)\n",
    "- `reason_for_loan`: The reason for requesting the loan (categorical)\n",
    "    - `DebtCon`: Debt consolidation\n",
    "    - `HomeImp`: Home Improvement\n",
    "- `occupation`: The individual's job title or profession (categorical)\n",
    "    - `Mgr`: Manager level\n",
    "    - `Office`: Office worker\n",
    "    - `Other`: All other professions\n",
    "    - `ProfExe`: Professional executive\n",
    "    - `Sales`: Sales professional\n",
    "    - `Self`: Self employed\n",
    "\n",
    "**Target:**\n",
    "- `bad_loan`: Indicates if the individual defaulted on the loan (binary)\n",
    "    - `1.0`: Default\n",
    "    - `0.0`: Loan repaid\n",
    "\n",
    "Using the features above, you will build a number of ensemble models throughout the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<a id = \"explore\"></a>\n",
    "### Quick Exploration\n",
    "\n",
    "Plot the distribution of the target variable: `bad_loan`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(hmeq['bad_loan']\n",
    "    .value_counts()\n",
    "    .plot(\n",
    "        kind='bar',\n",
    "        figsize=(8, 6),\n",
    "        title='Distribution of Target Variable',\n",
    "    )\n",
    ");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "The target variable, `bad_loan`, is **unbalanced** - meaning the variable contains about 4x more \"good\" loan instances than \"bad\" loan instances. This can present a problem since the **positive class** we want to predict is the \"bad\" loan class (`1.0`).\n",
    "\n",
    "Because of this unbalanced data, we will make sure that both our training set and testing set **maintain this ratio** of good:bad loans. This is acheived by using the `stratify` argument in the `train_test_split()` function, which was imported from the `sklearn.model_selection` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# separate target variable from features\n",
    "X = hmeq.drop('bad_loan', axis=1)\n",
    "y = hmeq['bad_loan'].copy()\n",
    "\n",
    "# partition data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                                    X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=RANDOM_SEED\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#top)\n",
    "<a id = \"q1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "#### Question 1\n",
    "\n",
    "> Given that our target variable (`bad_loan`) is unbalanced, what is the baseline classification accuracy for `X_train`?\n",
    "> - HINT: The [DummyClassifier](#https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html) may be helpful for this exercise.\n",
    "> - Use the `most_frequent` strategy of calculating the baseline accuracy\n",
    "> - Assign the baseline accuracy as a decimal (between [0.0-1.0]) to the identifier `ans1`. Round to 3 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Baseline Accuracy of X_train:\n",
      " =============================\n",
      "0.799\n"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "### \n",
    "### \n",
    "### Assign `ans1` \n",
    "dummy_clf = DummyClassifier(strategy='most_frequent')\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "ans1 = np.round(dummy_clf.score(X_train, y_train),3)\n",
    "\n",
    "\n",
    "### For verifying answer:\n",
    "print(\" Baseline Accuracy of X_train:\\n\",\"=\"*29)\n",
    "print(ans1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 01",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<a id = \"ensembles\"></a>\n",
    "## Ensemble Learning\n",
    "\n",
    "> _\"The idea of ensemble learning is to build a prediction model by combining the strengths of a collection of simpler base models._\"\n",
    ">\n",
    ">\n",
    "> _The Elements of Statistical Learning_, Chapter 16 [<sup>1</sup>](#ESL)\n",
    "\n",
    "\n",
    "\n",
    "Simply put, an ensemble method (model) is a collection of individual base models that work together to make a single prediction. For a single instance in a dataset, the ensemble algorithm will aggregate the predictions made by each individual base model and produce a single output prediction. This aggregation process varies across different ensemble architectures.\n",
    "\n",
    "The priciple idea behind using ensemble learning is that this committee of base models working together will outperform a single complex model. There are generally two families of ensemble methods:\n",
    "\n",
    "- **Averaging Methods**: Build several models independently and average their predictions.\n",
    "    - Examples: Bagging, Random Forest\n",
    "- **Boosting Methods**: Sequentially build a series of weak models, where each sequential model attempts to reduce the overall bias of the ensemble.\n",
    "    - Examples: AdaBoost, Gradient Boosted Trees\n",
    "\n",
    "There are many advantages to using ensemble methods.\n",
    "- **Interpretability**: Ensemble methods can be interpreted, although complex ensembles can sometimes make this difficult.\n",
    "- **High Performance**: Ensemble methods frequently win machine learning competitions\n",
    "- **Versatility**: Ensembles can be applied to regression or classification tasks\n",
    "\n",
    "### Ensemble Module: [`sklearn.ensemble`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble)\n",
    "\n",
    "The popular Python machine learning package, `scikit-learn`, maintains an excellent module dedicated to ensemble methods. This assignment will ask you to use 2-3 ensemble models from this module. You are encouraged to explore the others at your leisure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<a id = \"bagging\"></a>\n",
    "## Bagging: _Bootstrap Aggregation_\n",
    "> \n",
    "> \"_Bootstrap aggregation,_ or _bagging_, is a general-purpose procedure for reducing the variance of a statistical learning method\"\n",
    ">\n",
    "> _An Introduction to Statistical Learning: with Applications in R_ [<sup>4</sup>](#ISL)\n",
    "\n",
    "Bagging (short for *Bootstrap Aggregation*) is an averaging ensembling method that involves independently creating a committee of models, each fit on a different sample of training data. The different batches of training data for each base model is _boostrap sample_. Bootstrapping is discussed in the [Bootstrap Sampling](#bootstrap) section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<a id = \"bagging-works\"></a>\n",
    "### Why Bagging Works\n",
    "\n",
    "Decision Trees are a popular choice for the base estimator of a Bagging ensemble. As you may recall from learning about Decision Trees, it is very easy to overfit a decision tree to the training data. Furthermore, Decision Trees are known to be _unstable_, meaning a small change in the training data can result in a drastically different Decision Tree.\n",
    "\n",
    "A tree based Bagging ensemble takes advantage of these shortcomings by fitting each tree on a different bootstrap sample of the training data. Due to the unstable nature of Decision Trees, overfitting each tree to its particular bootstrapped sample will result in an ensemble with a diverse set of trees. While each individual tree is overfit to the data it was trained with, the variance of the overall ensemble is reduced due to this diverse set of trees.\n",
    "\n",
    "Since boostrapping is fundamental to Bagging, you will learn about that process next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<a id = \"bootstrap\"></a>\n",
    "### Bootstrap Sampling\n",
    "\n",
    "In short, a bootstrap is a randomly drawn sample *with replacement* in which the sample is of the same cardinality as the original dataset. The next question requires to build a function that will produce a single bootstrap DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#top)\n",
    "<a id = \"q2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "#### Question 2\n",
    "\n",
    "> Create a function `create_bootstrap_sample` that accepts take a Pandas `DataFrame` as a parameter and returns a bootstrap sample (also as a Pandas `DataFrame`).\n",
    "> - Name this function `create_bootstrap_sample` \n",
    "> - Use your function to create a single bootstrap sample using `X_train` as input\n",
    "> - Assign the bootstrap sample as a `pandas` `DataFrame` to the variable `ans2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Number of rows should be the same:\n",
      "Number of rows in X_train:   3781\n",
      "Number of rows in bootstrap: 3781\n",
      "------------------------------------------------------------\n",
      "Row labels of bootstrap should have repeated values:\n",
      "Contains repeat row labels: True\n"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "### \n",
    "### Create a function that will take df (a Pandas DataFrame) as an input\n",
    "### parameter and return a bootstrap sample. Assign the output sample to ans2.\n",
    "\n",
    "def create_bootstrap_sample(df):\n",
    "    sample_frame = np.random.choice(df.index, size=df.shape[0], replace=True)\n",
    "    return df.loc[sample_frame, :]\n",
    "\n",
    "ans2 = create_bootstrap_sample(X_train)\n",
    "\n",
    "### For verifying answer:\n",
    "print('-'*60)\n",
    "print('Number of rows should be the same:')\n",
    "print('Number of rows in X_train:  ', X_train.shape[0])\n",
    "print('Number of rows in bootstrap:', create_bootstrap_sample(X_train).shape[0])\n",
    "print('-'*60)\n",
    "print('Row labels of bootstrap should have repeated values:')\n",
    "print('Contains repeat row labels:', bool(~create_bootstrap_sample(X_train).index.is_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 02",
     "locked": true,
     "points": "7",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Now that you understand the bootstrapping process, your next task will be to build a generic Bagging ensemble. You will use the [`BaggingClassifier`](#https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) class from `scikit-learn` for the next question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#top)\n",
    "<a id = \"q3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "#### Question 3\n",
    "\n",
    "> Use the [`BaggingClassifier`](#https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier) class from Scikit-Learn, instantiate and fit a model with all the training data.\n",
    "> - Instantiate the classifier as `ans3a` using the default constructor parameters other than `random_state=42`, i.e., `BaggingClassifier(random_state=42)`.\n",
    "> - Fit the [`BaggingClassifier`](#https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier) instance `ans3a` using `X_train` and `y_train`.\n",
    "> - Make a prediction on `X_test` using the `predict` method. Assign the predicted output to the identifier `ans3b`.\n",
    "> - Score the accuracy on `X_test` & `y_test`; assign the score to the identifier `ans3c`.\n",
    ">\n",
    "> **Make sure to set the `random_state` parameter using `BaggingClassifier(random_state=42)` to ensure reproducibility!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model (should be sklearn):\n",
      "========================================\n",
      " BaggingClassifier(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=10, n_jobs=None, oob_score=False, random_state=42,\n",
      "         verbose=0, warm_start=False)\n",
      "\n",
      " First 5 predictions:\n",
      "========================================\n",
      " [0. 0. 1. ... 1. 0. 0.]\n",
      "\n",
      " Accuracy:\n",
      "========================================\n",
      " 0.8963602714373843\n"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "### \n",
    "### Instantiate and fit a BaggingClassifier model from sklearn \n",
    "### using default constructor arguments.\n",
    "\n",
    "### Instantiate and fit an instance of `BaggingClassifier` with `X_train` and `y_train`.\n",
    "### Use all default Constructor parameters, except for `random_state=42`. \n",
    "###     Ex: `BaggingClassifier(random_state=42)`\n",
    "### Assign the fit model to the identifier `ans3a`.\n",
    "### Make predictions on `X_test` using the `.predict()` method. Assign predictions to identifier `ans3b`.\n",
    "### Score the accuracy on `X_test` and assign the score to the identifier `ans3c`.\n",
    "### \n",
    "### Make sure to set `random_state` parameter to `BaggingClassifier(random_state=42)` for reproducibility!\n",
    "### \n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "ans3a = BaggingClassifier(random_state=42)\n",
    "ans3a.fit(X_train,y_train)\n",
    "ans3b = ans3a.predict(X_test)\n",
    "ans3c = ans3a.score(X_test,y_test)\n",
    "\n",
    "\n",
    "### For verifying answer:\n",
    "print((' Model (should be sklearn):\\n' + ('='*40) + '\\n {}\\n').format(ans3a))\n",
    "print((' First 5 predictions:\\n' + ('='*40) + '\\n {}\\n').format(ans3b))\n",
    "print((' Accuracy:\\n' + ('='*40) + '\\n {}').format(ans3c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 03",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Save this model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "bagging_model = ans3a       # bagging model\n",
    "bagging_test_preds = ans3b  # bagging test preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<a id = \"classification-metrics\"></a>\n",
    "### Classification Metrics Review\n",
    "An important consideration when evaluating classfier models is how well the model is predicting the \"positive\" class. In this case, the `1.0` value in the `bad_loan` column will be designated as the positive class.\n",
    "\n",
    "*Note*: Don't confuse the word \"positive\" in this case to mean \"optimistic\". The meaning of \"positive\" in this context simply means the focus is on identifying default loans (avoiding losses) rather than profitable loans.  The same would be true for a \"positive\" test result for an illness or a \"positive\" pregnancy test.\n",
    "\n",
    "Accuracy does not tell the whole story when evaluating a classifier model with **unbalanced** data. It's likely that the model is better at predicting the the majority class (`0.0`) because there are more samples for that class. We need to look at a few more evaluation metrics to understand how well the model predicts the \"positive\" class:\n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{accuracy} = \\frac{\\mathtt{tp} + \\mathtt{tn}}{\\mathtt{tn} + \\mathtt{tp} + \\mathtt{fn} + \\mathtt{fp}}\n",
    "\\end{align}\n",
    "$$\n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{recall} = \\frac{\\mathtt{tp}}{\\mathtt{tp} + \\mathtt{fn}}\n",
    "\\end{align}\n",
    "$$\n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{precision} = \\frac{\\mathtt{tp}}{\\mathtt{tp} + \\mathtt{fp}}\n",
    "\\end{align}\n",
    "$$\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "where $\\mathtt{tn}$, $\\mathtt{tp}$, $\\mathtt{fn}$, and $\\mathtt{fp}$ are the number of true negatives, true positives, false negatives, and false positives respectively.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<style>\n",
    "table {align:left;display:block}\n",
    "</style>\n",
    "\n",
    "\n",
    "| $$\\textbf{Metric}$$ | $$\\textbf{Meaning for this Model}$$ |\n",
    "| :------| :----- |\n",
    "| $$\\mathtt{tp}$$ | $$\\text{Number of actual bad loans predicted to be bad loans - the bank avoids losses.}$$ |\n",
    "| $$\\mathtt{tn}$$ | $$\\text{Number of actual good loans predicted to be good loans - the bank earns profit.}$$ |\n",
    "| $$\\mathtt{fp}$$ | $$\\text{Number of actual bad loans predicted to be good loans (false alarms!) - bank loses  money.)}$$ |\n",
    "| $$\\mathtt{fn}$$ | $$\\text{Number of actual good loans predicted to be bad loans - the bank misses opportunity to earn money.}$$ |\n",
    "\n",
    "\n",
    "<br>\n",
    "Depending on the use case, you may decide to optimize either $\\text{recall}$ or $\\text{precision}$. For example, consider a model that predicts if a tumor is malignant (positive class) or benign (negative class) from an ultrasound image. In this case, a $\\mathtt{fn}$ is much more costly than a $\\mathtt{fp}$, so $\\text{recall}$ is more important than $\\text{precision}$. The opposite situation could be true for a model that predicts when to replace expensive machinery parts. Machine learning metrics and business/health metrics are not always the same thing. Always consider the true cost you should optimize when model building.\n",
    "\n",
    "The `scikit-learn.metrics` module conveniently provides functions that will calculate these metrics:\n",
    "\n",
    "- [`classification_report()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html), \n",
    "- [`recall_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)\n",
    "- [`precision_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html).\n",
    "\n",
    "The below cell block demonstrates the `classification_report()` function to evaluate the entire Bagging ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.96      0.94      1295\n",
      "         1.0       0.80      0.64      0.71       326\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      1621\n",
      "   macro avg       0.86      0.80      0.83      1621\n",
      "weighted avg       0.89      0.90      0.89      1621\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    accuracy_score\n",
    ")\n",
    "print('Classification Report:\\n')\n",
    "print(classification_report(y_test, bagging_test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "`scikit-learn` provides users the ability to investigate the individual base models within an ensemble with the `.estimator_` attribute. Let's investigate the recall testing performance using each of the individual base models, which are `DecisionTreeClassifiers` by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def print_recall_scores(ensemble, feats, true_labels):\n",
    "    '''\n",
    "    Prints the recall scores for base estimators in a sklearn ensemble model.\n",
    "    '''\n",
    "    scores = []\n",
    "    for model_idx, model in enumerate(ensemble.estimators_):\n",
    "        if model_idx == 0:\n",
    "            print('='*40)\n",
    "        preds = model.predict(feats)\n",
    "        scores.append(recall_score(true_labels, preds))\n",
    "        model_recall = np.round(recall_score(true_labels, preds), 5)\n",
    "        print(f'Recall for Base Model {model_idx+1}:\\t', model_recall)\n",
    "        if model_idx < (len(ensemble.estimators_) - 1):\n",
    "            print('-'*40)\n",
    "        else:\n",
    "            print('='*40)\n",
    "    ensemble_preds = ensemble.predict(feats)\n",
    "    print(\"Mean Recall Score:\\t\\t\", np.round(np.array(scores).mean(), 5))\n",
    "    print(\"Std Deviation:\\t\\t\\t\", np.round(np.array(scores).std(), 5))\n",
    "    print(\"Range:\\t\\t\\t\\t\", np.round(np.array(scores).ptp(), 5))\n",
    "    print(f'Overall Recall for model:\\t {np.round(recall_score(y_test, ensemble_preds), 5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Recall for Base Model 1:\t 0.61963\n",
      "----------------------------------------\n",
      "Recall for Base Model 2:\t 0.62577\n",
      "----------------------------------------\n",
      "Recall for Base Model 3:\t 0.6227\n",
      "----------------------------------------\n",
      "Recall for Base Model 4:\t 0.61043\n",
      "----------------------------------------\n",
      "Recall for Base Model 5:\t 0.59816\n",
      "----------------------------------------\n",
      "Recall for Base Model 6:\t 0.64417\n",
      "----------------------------------------\n",
      "Recall for Base Model 7:\t 0.60429\n",
      "----------------------------------------\n",
      "Recall for Base Model 8:\t 0.61656\n",
      "----------------------------------------\n",
      "Recall for Base Model 9:\t 0.66258\n",
      "----------------------------------------\n",
      "Recall for Base Model 10:\t 0.61963\n",
      "========================================\n",
      "Mean Recall Score:\t\t 0.62239\n",
      "Std Deviation:\t\t\t 0.01791\n",
      "Range:\t\t\t\t 0.06442\n",
      "Overall Recall for model:\t 0.64417\n"
     ]
    }
   ],
   "source": [
    "print_recall_scores(bagging_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Notice that the recall on each individual tree is very similar.\n",
    "\n",
    "The generic Bagging ensemble method will now be compared to a Random Forest model, which is a variant of the standard Bagging method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<a id = \"random-forest\"></a>\n",
    "### Random Forest\n",
    "\n",
    "> \"Random Forests is a substantial modification of bagging that builds a large collection of _de-correlated_ trees, and averages them.\"\n",
    "> \n",
    "> _The Elements of Statistical Learning_, Chapter 15 [<sup>1</sup>](#ESL)\n",
    "\n",
    "When constructing a tree within a bagging ensemble, all input features are considered to determine the best split. If the data contains one or two dominant features, those dominant features are always selected first in every tree within the ensemble, resulting in a high correlation among the trees.\n",
    "\n",
    "The Random Forest algorithm further reduces the model error due to variance by de-correlating each tree within the ensemble. This is acheived by considering only a sample of features from the available feature set at each split in the decision tree. This procedure results in a higher diversity of trees within the ensemble, which can reduce the overall variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#top)\n",
    "<a id = \"q4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "#### Question 4\n",
    "\n",
    "> Your task now is to build a Random Forest model using the [`RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) class from `scikit-learn`.\n",
    ">\n",
    "> - Instantiate a `RandomForestClassifier` using the constructor arguments: `n_estimators=10`, `max_features=7` and `random_state=42`, i.e., `RandomForestClassifier(n_estimators=10, max_features=7, random_state=42)`. Assign the resulting object to the identifier `ans4a`.\n",
    "> - Fit the object `ans4a` using the training data `X_train` and `y_train`.\n",
    "> - Make predictions on `X_test`. Assign the resulting predictions to `ans4b` as a 1D numpy array.\n",
    "> - Calculate the recall using the testing data and assign the value to `ans4c`. Round to 4 decimal places.\n",
    "> - Calculate the precision using the testing data and assign the value to `ans4d`. Round to 4 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type for answer for ans4a: <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "Type for answer for ans4b: <class 'numpy.ndarray'>\n",
      "Type for answer for ans4c: <class 'numpy.float64'>\n",
      "Type for answer for ans4d: <class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "### \n",
    "### Instantiate and fit a `RandomForestClassifier` model to `X_train`.\n",
    "### Use the following constructor arguments: `max_features=7` and `random_state=42`\n",
    "### Ex: RandomForestClassifier(max_features=7, random_state=42)\n",
    "### Assign the fitted model to the identifier `ans4a`\n",
    "### Make predictions on `X_test`. Assign the predictions to `ans4b` as a 1D numpy array.\n",
    "### Calculate the recall on testing and assign the value to `ans4c`. Round to 4 decimal places.\n",
    "### Calculate the precision on testing and assign the value to `ans4d`. Round to 4 decimal places.\n",
    "### \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "ans4a = RandomForestClassifier(n_estimators=10, max_features=7, random_state=42)\n",
    "ans4a.fit(X_train, y_train)\n",
    "ans4b = ans4a.predict(X_test)\n",
    "ans4c = np.round(recall_score(y_test,ans4b),4)\n",
    "ans4d = np.round(precision_score(y_test,ans4b),4)\n",
    "\n",
    "### For verifying answer:\n",
    "print('Type for answer for ans4a:', type(ans4a))\n",
    "print('Type for answer for ans4b:', type(ans4b))\n",
    "print('Type for answer for ans4c:', type(ans4c))\n",
    "print('Type for answer for ans4d:', type(ans4d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 04",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Save the Random Forest model for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "rfc = ans4a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "The implementation of Random Forest should de-correlate the trees from each other, resulting in a higher variance in performance on `X_test` when we look at each individual tree in the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Recall for Base Model 1:\t 0.63804\n",
      "----------------------------------------\n",
      "Recall for Base Model 2:\t 0.65031\n",
      "----------------------------------------\n",
      "Recall for Base Model 3:\t 0.64724\n",
      "----------------------------------------\n",
      "Recall for Base Model 4:\t 0.63804\n",
      "----------------------------------------\n",
      "Recall for Base Model 5:\t 0.60123\n",
      "----------------------------------------\n",
      "Recall for Base Model 6:\t 0.6319\n",
      "----------------------------------------\n",
      "Recall for Base Model 7:\t 0.64724\n",
      "----------------------------------------\n",
      "Recall for Base Model 8:\t 0.59509\n",
      "----------------------------------------\n",
      "Recall for Base Model 9:\t 0.68098\n",
      "----------------------------------------\n",
      "Recall for Base Model 10:\t 0.61963\n",
      "========================================\n",
      "Mean Recall Score:\t\t 0.63497\n",
      "Std Deviation:\t\t\t 0.02376\n",
      "Range:\t\t\t\t 0.08589\n",
      "Overall Recall for model:\t 0.63804\n"
     ]
    }
   ],
   "source": [
    "print_recall_scores(rfc, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "The `BaggingClassifier` and `RandomForestClassifier` have similar overall recall scores, but the variance of recall scores among the individual Decision Trees is higher in the Random Forest due to the additional sampling of the feature space as the trees are constructed. The trees within the Random Forest model are more diverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<a id = \"extra-trees\"></a>\n",
    "### ExtraTreesClassifier\n",
    "\n",
    "It's possible to make the tree ensemble even more random by using _extremely randomized trees_. In extremely randomized trees, each base learner is constructed using a bootstrap sample of the training data (just like in bagging). At each split during the construction of a tree, a subset of features are selected as candidates to make the split (just like Random Forest), but instead of finding the optimal threshold to split each candidate feature, a random threshold is chosen. The feature with the best randomly selected threshoold is then chosen as the splitting rule at that node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#top)\n",
    "<a id = \"q5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "#### Question 5\n",
    "\n",
    "> Your task is to build an extremely randomized tree model using the [`ExtraTreesClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier)  class from `scikit-learn`.\n",
    ">\n",
    "> - Instantiate and fit an `ExtraTreesClassifier` model to the training data (using the constructor arguments `n_estimators=10`, `max_features=7`, and `random_state=42`).\n",
    "> - Assign the model to the identifier `ans5a`.\n",
    "> - Make predictions on `X_test`. Assign the predictions to `ans5b` as a 1D numpy array.\n",
    "> - Calculate the recall on testing and assign the value to `ans5c`. Round to 4 decimal places.\n",
    "> - Calculate the precision on testing and assign the value to `ans5d`. Round to 4 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type for answer for ans5a: <class 'sklearn.ensemble.forest.ExtraTreesClassifier'>\n",
      "Type for answer for ans5b: <class 'numpy.ndarray'>\n",
      "Recall :\t 0.681\n",
      "Precision :\t 0.9569:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "### \n",
    "### Instantiate and fit a `ExtraTreesClassifier` model to `X_train`.\n",
    "### Use the following constructor arguments: `max_features=7` and `random_state=42`\n",
    "### Ex: ExtraTreesClassifier(max_features=7, random_state=42)\n",
    "### Assign the fitted model to the identifier `ans5a`\n",
    "### Make predictions on `X_test`. Assign the predictions to `ans5b` as a 1D numpy array.\n",
    "### Calculate the recall on testing and assign the value to `ans5c`. Round to 4 decimal places.\n",
    "### Calculate the precision on testing and assign the value to `ans5d`. Round to 4 decimal places.\n",
    "### \n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "ans5a = ExtraTreesClassifier(max_features=7, random_state=42)\n",
    "ans5a.fit(X_train,y_train)\n",
    "ans5b = ans5a.predict(X_test)\n",
    "ans5c = np.round(recall_score(y_test,ans5b),4)\n",
    "ans5d = np.round(precision_score(y_test,ans5b),4)\n",
    "\n",
    "\n",
    "### For verifying answer:\n",
    "print('Type for answer for ans5a:', type(ans5a))\n",
    "print('Type for answer for ans5b:', type(ans5b))\n",
    "print('Recall :\\t {}'.format(ans5c))\n",
    "print('Precision :\\t {}:'.format(ans5d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 05",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Save the Extra Trees model for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "extra_tree_model = ans5a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#top)\n",
    "<a id = \"q6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<a id = \"oob\"></a>\n",
    "### Out-of-Bag Evaluation\n",
    "\n",
    "Through the process of bootstrap sampling, some instances can be sampled multiple times, while other instances are left out of the sample altogether. The samples that are left out of the bootstrap sample are known as \"out-of_bag\" (oob) samples. Since these observations are not used during the training of a particular base learner, the oob instances can be used as testing samples. This eliminates the need to withhold data from training for the purpose of evaluation.\n",
    "\n",
    "Ensemble models in `scikit-learn` contain a parameter in the model constructors called `oob_score`. Setting `oob_score=True` will enable this implementation. After fitting the model, the evaluation results can be found in the attribute `.oob_score_`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "#### Question 6\n",
    "\n",
    "> Your task is to create another `RandomForestClassifier` model using `X_train`, but \n",
    "this time setting the `oob_score` parameter to `True`. You will then compare the \n",
    "oob accuracy score to the accuracy of the model on `X_test`.\n",
    "> \n",
    "> - Instantiate and fit a `RandomForestClassifier` model to `X_train`.\n",
    "Use the following constructor arguments: `n_estimators=100`, `max_features=7`, `oob_score=True`, and `random_state=42`\n",
    "> - Assign the fitted model to the identifier `ans6a`.\n",
    "> - Assign the `oob_score_` (accuracy) to the identifier `ans6b`. Round to 4 decimal places.\n",
    "> - Calculate the accuracy on `X_test` and assign the value to `ans6c`. Round to 4 decimal places.\n",
    "> - Calculate the absolute difference between the oob accuracy (`ans6b`) and testing accuracy (`ans6c`) & assign the value computed to the identifier `ans6d`. Round to 4 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type for answer for ans6a:\t RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=7, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      "            oob_score=True, random_state=42, verbose=0, warm_start=False)\n",
      "OOB Score:\t\t\t 0.9127\n",
      "Testing Accuracy:\t\t 0.9155\n",
      "Acc. Difference:\t\t 0.0028000000000000247\n"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "### \n",
    "### Instantiate and fit a `RandomForestClassifier` model to `X_train`.\n",
    "### Use the following constructor arguments: \n",
    "###     - `n_estimators=100`\n",
    "###     - `max_features=7`,\n",
    "###     - `oob_score=True`\n",
    "###     - `random_state=42`\n",
    "### Assign the fitted model to the identifier `ans6a`.\n",
    "### Assign the oob_score_ (accuracy) to the identifier `ans6b`. Round to 4 decimal places.\n",
    "### Calculate the accuracy on testing and assign the value to `ans6c`. Round to 4 decimal places.\n",
    "### Calculate the difference between the oob_score_ (`ans6b`) and testing accuracy (`ans6c`).\n",
    "### Assign the absolute difference between `ans6b` and `ans6c` to the identifier `ans6d`. Round to 4 decimal places.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "ans6a = RandomForestClassifier(n_estimators=100,max_features=7,oob_score=True,random_state=42)\n",
    "ans6a.fit(X_train,y_train)\n",
    "ans6b = np.round(ans6a.oob_score_,4)\n",
    "prediction = ans6a.predict(X_test)\n",
    "ans6c = np.round(accuracy_score(y_test,prediction),4)\n",
    "ans6d = abs(ans6b-ans6c)\n",
    "\n",
    "\n",
    "### For verifying answer:\n",
    "print('Type for answer for ans6a:\\t', ans6a)\n",
    "print('OOB Score:\\t\\t\\t', ans6b)\n",
    "print('Testing Accuracy:\\t\\t', ans6c)\n",
    "print('Acc. Difference:\\t\\t', ans6d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 06",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#top)\n",
    "<a id = \"q7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<a id = \"boosting\"></a>\n",
    "## Boosting\n",
    "> \"Boosting is one of the most powerful learning ideas introduced in the last twenty years.\"\n",
    ">\n",
    ">\n",
    "> _The Elements of Statistical Learning_, Chapter 10 [<sup>1</sup>](#ESL)\n",
    "\n",
    "\n",
    "Boosting is fundamentally different than bagging. Bagging ensembles attempt to reduce overall _variance_ by fitting models independently on bootstapped samples. Boosting methods attempt to reduce _bias_ by sequentially improving the overall predictions. That is, base learner $m$ attempts to correct the predictions produced by the base learner $m-1$, for $m=1$ to $M$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "#### Question 7\n",
    "\n",
    "> **True or False**\n",
    "> \n",
    "> Determine if the following statement is True or False. Provide your answer as a boolean literal (`True` or `False`) to the identifier `ans7`.\n",
    "> \n",
    ">\n",
    "> _Each base model in a Boosting Ensemble should be overfit to the data, just as in Bagging._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### True or False: Each base model in a Boosting Ensemble should be \n",
    "### overfit to the data, just as in Bagging.\n",
    "### \n",
    "### Provide your answer as a boolean literal (True or False) to\n",
    "## the identifier ans7.\n",
    "\n",
    "ans7 = False\n",
    "\n",
    "\n",
    "### For verifying answer:\n",
    "if type(ans7)!= bool:\n",
    "    print('Please provide your response as a boolean literal (True or False)')\n",
    "    print('Your response type: {}.'.format(type(ans7)))\n",
    "    print('Correct response type: {}.'.format(bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 07",
     "locked": true,
     "points": "3",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<a id = \"adaboost\"></a>\n",
    "### AdaBoost\n",
    "\n",
    "One of the prototypical boosting algorithms was Adaptive Boosting, or AdaBoost. The AdaBoost algorithm begins by assigning equal weights to every observation in the training data. The base learner in the ensemble is fit on the training data and produces predictions. Based on these predictions, the weights are adjusted to increase the for misclassified observations and reduced for correctly classified observations. Each model repeats this procedure of making predictions and updating the weights accordingly.\n",
    "\n",
    "The principle idea behind AdaBoost is that difficult to predict cases get assigned a heavier and heavier weight so that base learners futher down the ensemble become better at predicting them. We will now practice building and evaluating an AdaBoost model in `scikit-learn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#top)\n",
    "<a id = \"q8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "#### Question 8\n",
    "\n",
    "> Your task is to build two AdaBoost models with TRAINING data using the [`AdaBoostClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) class from `scikit-learn` and report the best `recall_score()` on TESTING.\n",
    ">\n",
    "> \"MODEL 1\" Hyperparameters:\n",
    "> - Use `DecisionTreeClassifier(max_depth=2)` as base estimator\n",
    "> - Set `n_estimators=500`\n",
    "> - Set `random_state=42`\n",
    "> - Set `learning_rate = 0.05`\n",
    ">\n",
    "> \"MODEL 2\" Hyperparameters::\n",
    "> - Use `DecisionTreeClassifier(max_depth=2)` as base estimator\n",
    "> - Set `n_estimators=20`\n",
    "> - Set `random_state=42`\n",
    "> - Set `learning_rate = 0.5`\n",
    ">\n",
    "> - Calculate the \"MODEL 1\" recall on testing and assign the value to `ans8a`. Round to 4 decimal places.\n",
    "> - Calculate the \"MODEL 2\" recall on testing and assign the value to `ans8b`. Round to 4 decimal places.\n",
    "> - Assign `ans8c` to the string \"MODEL 1\" or \"MODEL 2\" depending on which model had a better recall on TESTING."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winning Model:\t MODEL 1\n",
      "MODEL 1 Recall:\t 0.6933\n",
      "MODEL 2 Recall:\t 0.6411:\n"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "### \n",
    "### \n",
    "### Assign `ans8` \n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "base = DecisionTreeClassifier(max_depth=2)\n",
    "\n",
    "ada1 = AdaBoostClassifier(base,n_estimators=500,random_state=42,learning_rate = 0.05)\n",
    "ada1.fit(X_train, y_train)\n",
    "prediction1 = ada1.predict(X_test)\n",
    "ans8a = np.round(recall_score(y_test,prediction1),4)\n",
    "\n",
    "ada2 = AdaBoostClassifier(base,n_estimators=20,random_state=42,learning_rate = 0.5)\n",
    "ada2.fit(X_train, y_train)\n",
    "prediction2 = ada2.predict(X_test)\n",
    "ans8b = np.round(recall_score(y_test,prediction2),4)\n",
    "\n",
    "ans8c = 'MODEL 1'\n",
    "\n",
    "\n",
    "### For verifying answer:\n",
    "print('Winning Model:\\t {}'.format(ans8c))\n",
    "print('MODEL 1 Recall:\\t {}'.format(ans8a))\n",
    "print('MODEL 2 Recall:\\t {}:'.format(ans8b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 08",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<a id = \"gbt\"></a>\n",
    "### Gradient Boosted Trees (GBT)\n",
    "\n",
    "An extremely popular boosting algorithm is Gradient Booosted Trees. Instead of re-weighting observations based on the prediction performance, Gradient Boosted Trees attempt to correct the residuals errors of the preceding models. The initial base estimator is fit on the training data. From then on, subsequent model is fit on the previous model's residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#top)\n",
    "<a id = \"q9\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "#### Question 9\n",
    "\n",
    "> Your task is to create now use `GradientBoostingClassifier` model using `X_train` & `y_train`.\n",
    ">\n",
    "> - Instantiate and fit a `GradientBoostingClassifier` model to the training data (using default constructor arguments except for `random_state=42`).\n",
    "> - Assign the model to the identifier `ans9a`.\n",
    "> - Make predictions on `X_test`. Assign the predictions to `ans9b` as a 1D numpy array.\n",
    "> - Calculate the recall on testing and assign the value to `ans9c`. Round to 4 decimal places.\n",
    "> - Calculate the precision on testing and assign the value to `ans9d`. Round to 4 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type for answer for ans9a: <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>\n",
      "Type for answer for ans9b: <class 'numpy.ndarray'>\n",
      "Recall :\t 0.6595\n",
      "Precision :\t 0.8333:\n"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "### Create a GradientBoostingClassifier as described above (assigning the\n",
    "###   appropriate objects to ans9a through ans9d).\n",
    "### YOUR SOLUTIONS HERE:\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "ans9a = GradientBoostingClassifier(random_state=42)\n",
    "ans9a.fit(X_train,y_train)\n",
    "ans9b = ans9a.predict(X_test)\n",
    "ans9c = np.round(recall_score(y_test,ans9b),4)\n",
    "ans9d = np.round(precision_score(y_test,ans9b),4)\n",
    "\n",
    "\n",
    "### For verifying answer:\n",
    "print('Type for answer for ans9a:', type(ans9a))\n",
    "print('Type for answer for ans9b:', type(ans9b))\n",
    "print('Recall :\\t {}'.format(ans9c))\n",
    "print('Precision :\\t {}:'.format(ans9d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 09",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Save this model for the next question on feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "gbt_model = ans9a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<a id = \"feature-importance\"></a>\n",
    "## Feature Importance\n",
    "\n",
    "A fantastic characteristic of many ensemble models is that you have the ability to interpret the feature importance. As you learned with Decision Trees, the most important features are selected first during the construction of a tree. Using the gini or information gain generated from using a feature to make a split, a feature importance score can be calculated.\n",
    "\n",
    "In the case of ensembles, these feature importance scores are aggregated over all of the trees within the ensemble. `scikit-learn` conveniently calculates a `.feature_importance_` score for many of their ensemble implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#top)\n",
    "<a id = \"q10\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "#### Question 10\n",
    "\n",
    "> You task find the most important feature from the fitted `GradientBoostingClassifier` model from Question 9.\n",
    "> - Based on the `feature_importances_` attribute, assign the top feature to the identifier `ans10` as a string literal.\n",
    "\n",
    "_Note_: You may need to research how to map the feature importance scores stored in `gbt_model.feature_importances_` back to their respective column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('reason_for_loan_DebtCon', 0.0), ('reason_for_loan_HomeImp', 0.0), ('occupation_Other', 0.0005528034230555963), ('occupation_Self', 0.0005672201577792003), ('occupation_Mgr', 0.0018724613813850632), ('occupation_Sales', 0.0028872939335593335), ('occupation_Office', 0.005593242294894303), ('occupation_ProfExe', 0.006642313621074149), ('years_at_job', 0.01525166017360266), ('num_recent_cl', 0.023997960826799607), ('amt_due_on_mort', 0.029238057065466595), ('value_of_property', 0.03852189715013419), ('loan_request', 0.03881509481574239), ('num_of_cl', 0.04333319867339297), ('oldest_cl_age', 0.07476710610561932), ('num_derog_reports', 0.0919714006039573), ('num_delinq_lines', 0.13455923859049168), ('debt_to_inc_ratio', 0.4914290511830456)]\n",
      "Top Feature:\tdebt_to_inc_ratio\n"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "### Identify the most important feature from gbt_model as found from Question 9.\n",
    "### Assign your result as a Python string to ans10.\n",
    "### YOUR SOLUTION HERE:\n",
    "print(sorted(list(zip(X_train.columns,gbt_model.feature_importances_)), key=lambda x: x[1]))\n",
    "ans10 = \"debt_to_inc_ratio\"\n",
    "\n",
    "\n",
    "### For verifying answer:\n",
    "print(f'Top Feature:\\t{ans10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 10",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<a id = \"references\"></a>\n",
    "## References\n",
    "\n",
    "Several explanations and examples in this assignment came from the following excellent resources.\n",
    "<a id = \"ESL\"></a>\n",
    "1. Hastie, Trevor, et al. _The Elements of Statistical Learning, Second Edition: Data Mining, Inference, and Prediction_. Springer, 2009.\n",
    "<a id = \"hands-on\"></a>\n",
    "2. Geron Aurelien. _Hands-on Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems_. O'Reilly Media, 2017.\n",
    "<a id = \"pml\"></a>\n",
    "3. Raschka, Sebastian. _Python Machine Learning_. Packt, 2015.\n",
    "4. An Introduction to Statistical Learning: with Applications in R. _An Introduction to Statistical Learning: with Applications in R_, by Gareth James et al., Springer, 2017, pp. 316316.\n",
    "\n",
    "<a id = \"hmeq\"></a>\n",
    "Dataset:\n",
    "4. HMEQ Dataset: https://www.kaggle.com/ajay1735/hmeq-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#top)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 [3.6]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
