{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Naive Bayes Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "_Author: Dhavide Aruliah_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Assignment Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "- [Question 1: Computing Discrete Probabilities](#q-club-black)\n",
    "- [Question 2: Computing Conditional probabilities](#q-cond-p)\n",
    "- [Question 3: Reasoning about Spam Messages](#q-spam)\n",
    "- [Question 4: Preparing the SMS Messaging Data](#q-preparing)\n",
    "- [Question 5: Getting priors](#q-priors)\n",
    "- [Question 6: Getting likelihoods](#q-likelihoods)\n",
    "- [Question 7: Computing smoothed likelihoods](#q-smoothed)\n",
    "- [Question 8: Predicting spam](#q-predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### EXPECTED TIME 2.0 HRS  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Activities in this Assignment\n",
    "\n",
    "This assignment provides an overview of *Naive Bayes classifiers* as an approach to classification problems in supervised learning. In spite of the \"naive\" assumptions involved, it works very well in practice particularly for text analysis in, for instance, spam filtering or document classification. As such, this assignment is built around a very simple model of spam filtering to get a sense of how naive Bayes classification really works.\n",
    "\n",
    "The primary goals are:\n",
    "+ to review notions of probability as related to Bayes' theorem (notably independent events & conditional probability).\n",
    "+ to practice the application of Bayes' theorem for probabilistic reasoning.\n",
    "+ to develop a (highly simplified) model of text analysis for spam classification using the naive Bayes classification framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Reminders from Discrete Probability\n",
    "\n",
    "For finite sets, probabilities of distinct events can be modeled using *sets*.\n",
    "\n",
    "As an example, consider a standard deck of playing cards. There are 52 cards in a deck with four suits (clubs (♣), diamonds (♢), hearts (♡), and spades (♠)) each with one of thirteen ranks (two through ten, jack, queen, king, and ace). We can represent a deck in Python by a collection of tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['♣', '♢', '♡', '♠']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('2', '♣'), ('2', '♢'), ('2', '♡'), ('2', '♠'), ('3', '♣')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suits = ['♣', '♢', '♡', '♠']\n",
    "print(suits)\n",
    "\n",
    "ranks = ['2', '3', '4', '5', '6', '7', '8' ,'9', '10', 'J', 'Q', 'K', 'A']\n",
    "deck = [ (r,s) for r in ranks for s in suits ]\n",
    "deck[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('6', '♠'), ('K', '♢'), ('9', '♣'), ('3', '♢'), ('7', '♣')]\n"
     ]
    }
   ],
   "source": [
    "# Put the deck into random order.\n",
    "import random\n",
    "random.shuffle(deck)\n",
    "print(deck[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An *event* is any subset of a set of possible outcomes. For instance, let $E_{\\text{black}}$ is the event of drawing a black card from the deck and let $E_{\\text{club}}$ be the event of drawing a club from the deck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.25\n"
     ]
    }
   ],
   "source": [
    "E_black = {card for card in deck if ((card[1]=='♠') or (card[1]=='♣'))}\n",
    "E_club = {card for card in deck if (card[1]=='♣')}\n",
    "print(len(E_black)/len(deck), len(E_club)/len(deck))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Assignment-Contents)\n",
    "<a id=\"q-club-black\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "### Question 1: Computing Discrete Probabilities\n",
    "\n",
    "Your first task is to answer a few questions about probability and a standard deck of cards.\n",
    "\n",
    "+ What is $p(E_{\\text{black}})$, the probability of drawing a single black card (i.e., a card that is either of the club suit or the spades suit) from the deck? Assign your answer to `p_black`.\n",
    "  + You can simply assign the number or you can compute it empirically using the Python sets `E_black` & `deck`.\n",
    "+ What is $p(E_{\\text{club}})$, the probability of drawing a single club (i.e., a card from the club suit) from the deck? Assign your answer to `p_club`.\n",
    "  + You can simply assign the number or you can compute it empirically using the Python sets `E_club` & `deck`.\n",
    "+ **True** or **False**:  the events $E_{\\text{black}}$ & $E_{\\text{club}}$ are independent. Assign your answer as a Python boolean literal `True` or `False` to `independent_club_black`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_black = 0.5\n",
      "p_club = 0.25\n",
      "independent_club_black = False\n"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "### QUESTION 1:\n",
    "### Assign values to p_black, p_club, and independent_club_black as described above.\n",
    "### YOUR SOLUTION HERE:\n",
    "p_black = len(E_black) / len(deck)\n",
    "p_club = len(E_club) / len(deck)\n",
    "independent_club_black = False\n",
    "### For verifying answer:\n",
    "print('p_black = {}'.format(p_black))\n",
    "print('p_club = {}'.format(p_club))\n",
    "print('independent_club_black = {}'.format(independent_club_black))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 01",
     "locked": true,
     "points": "6",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Assignment-Contents)\n",
    "<a id=\"q-cond-p\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "### Question 2: Computing Conditional probabilities\n",
    "\n",
    "+ Suppose I draw a card from the deck and I tell you it is a black card. What is the probability that that the card drawn is also a club? That is, what is the *conditional probability* that $p(E_{\\text{club}}\\,|\\,E_{\\text{black}})$?\n",
    "  + Assign the value of $p(E_{\\text{club}}\\,|\\,E_{\\text{black}})$ to `p_club_black` as a standard Python floating-point numeric value.\n",
    "+ Alternatively, suppose I draw a card from the deck and I tell you it is a club, i.e., a card from the club suit. What is the probability that that the card drawn is also black? That is, what is the *conditional probability* that $p(E_{\\text{black}}\\,|\\,E_{\\text{club}})$?\n",
    "  + Assign the value of $p(E_{\\text{black}}\\,|\\,E_{\\text{club}})$ to `p_black_club` as a standard Python floating-point numeric value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_club_black = 0.5\n",
      "p_black_club = 1.0\n"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "### QUESTION 2:\n",
    "### Assign numeric values to p_club_black & p_black_club as described above.\n",
    "### YOUR SOLUTION HERE:\n",
    "p_club_int_black = len(E_club.intersection(E_black))/len(deck)\n",
    "p_club_black = p_club_int_black / p_black\n",
    "p_black_club = p_club_int_black / p_club\n",
    "### For verifying answer:\n",
    "print('p_club_black = {}'.format(p_club_black))\n",
    "print('p_black_club = {}'.format(p_black_club))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 02",
     "locked": true,
     "points": "4",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Assignment-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Having reviewed a little about independent and conditional probabilities, remember *Bayes' theorem*:\n",
    "\n",
    "$$\\displaystyle{\\boxed{p(A\\,|\\,B) = \\frac{p(B\\,|\\,A) p(A)}{p(B)}}}$$\n",
    "\n",
    "+ $p(A\\,|\\,B)$ is the \"*posterior* probability of $A$ given $B$\";\n",
    "+ $p(B\\,|\\,A)$ is the \"*likelihood* of $B$ given $A$\";\n",
    "+ $p(A)$ is the \"*prior* probability of $A$\"; and\n",
    "+ $p(B)$ is the \"*evidence*\" (normalizing factor).\n",
    "\n",
    "The next questions require you to apply Bayes' theorem to reason about spam messages. Remember, the goal is to identify messages as *spam* (not wanted, undesirable) or *ham* (i.e., the opposite of spam messages)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Assignment-Contents)\n",
    "<a id=\"q-spam\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "### Question 3: Reasoning about Spam Messages\n",
    "\n",
    "Assume in the following that you have a training set of 2,000 messages known to be spam and 1,000 messages known to be ham (i.e., not spam). Suppose further that the word \"bargain\" occurs in 250 of the spam messages and 5 of the ham messages.\n",
    "\n",
    "+ Assume the empirical prior probability of an incoming message being ham or spam is provided by the respective fractions of ham or spam messages in the training set.\n",
    " + Assign the (estimated) prior probability of spam to `prior_spam` (i.e., $p(\\text{spam})$).\n",
    " + Assign the (estimated) prior probability of ham to `prior_ham`  (i.e., $p(\\text{ham})$).\n",
    "+ Assume the empirical likelihood of the word \"bargain\" occurring in a message known to be spam (respectively, ham) is given by the counts above.\n",
    " + Assign the (estimated) likelihood of \"bargain\" occurring in an incoming spam message to `likelihood_bargain_spam` (i.e., $p(\\text{bargain}\\,|\\,\\text{spam})$).\n",
    " + Assign the (estimated) likelihood of \"bargain\" occurring in an incoming ham message to `likelihood_bargain_ham` (i.e., $p(\\text{bargain}\\,|\\,\\text{ham})$).\n",
    "+ Finally, combine the preceding computations to estimate the *posterior* probability of an incoming message being spam given that it contains the word \"bargain\" (that is, $p(\\text{spam}\\,|\\,\\text{bargain})$).\n",
    " + Assign the posterior probability $p(\\text{spam}\\,|\\,\\text{bargain})$ to `posterior_spam_bargain`.\n",
    "+ Assign all the values computed here to Python floating-point values up to three decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prior_spam: 0.667\n",
      "prior_ham:  0.333\n",
      "likelihood_bargain_spam: 0.125\n",
      "likelihood_bargain_ham : 0.005\n",
      "posterior_spam_bargain: 0.980\n"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "### QUESTION 3:\n",
    "### Assign floating-point values to prior_spam, prior_ham, likelihood_bargain_spam,\n",
    "###   likelihood_bargain_ham, and posterior_spam_bargain as described above.\n",
    "### Provide results accurate to at least 3 decimal places (i.e., an absolute tolerance of 1.0e-3).\n",
    "### YOUR SOLUTION HERE:\n",
    "n_spam = 2000\n",
    "n_ham = 1000\n",
    "prior_spam = n_spam / (n_spam + n_ham)\n",
    "prior_ham = n_ham / (n_spam + n_ham)\n",
    "likelihood_bargain_spam = 250 / n_spam\n",
    "likelihood_bargain_ham = 5 / n_ham\n",
    "posterior_spam_bargain = likelihood_bargain_spam * prior_spam / (likelihood_bargain_spam * prior_spam + likelihood_bargain_ham * prior_ham)\n",
    "\n",
    "### For verifying answer:\n",
    "print('prior_spam: {:5.3f}'.format(prior_spam))\n",
    "print('prior_ham:  {:5.3f}'.format(prior_ham))\n",
    "print('likelihood_bargain_spam: {:5.3f}'.format(likelihood_bargain_spam))\n",
    "print('likelihood_bargain_ham : {:5.3f}'.format(likelihood_bargain_ham))\n",
    "print('posterior_spam_bargain: {:5.3f}'.format(posterior_spam_bargain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 03",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Assignment-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Filtering Spam from SMS Messages\n",
    "\n",
    "For the next questions, you'll work with a dataset from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php), namely the [*SMS Spam Collection*]( https://archive.ics.uci.edu/ml/datasets/sms+spam+collection), a public set of labeled SMS messages that have been collected for mobile phone spam research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Assignment-Contents)\n",
    "<a id=\"q-preparing\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "### Question 4: Preparing the SMS Messaging Data\n",
    "\n",
    "Your task now is to load the SMS messaging data into a Pandas DataFrame.\n",
    "\n",
    "+ The data is stored in a file whose location is provided for you as `FILE_PATH`. Use the function `pd.read_csv` with the options `sep=\"\\t\"` and `header=None`.\n",
    "+ Assign the resulting `DataFrame` object to the identifier `messages`.\n",
    "+ Give the DataFrame meaningful column headers by assigning the list `['target', 'msg']` to `messages.columns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                                msg\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### GRADED\n",
    "### QUESTION 4:\n",
    "### Prepare the dataframe messages as specified above. \n",
    "###\n",
    "# Necessary imports\n",
    "import numpy as np, pandas as pd\n",
    "FILE_PATH = 'data/SMSSpamCollection.txt'\n",
    "### YOUR SOLUTION HERE:\n",
    "messages = pd.read_csv(FILE_PATH,sep='\\t',header=None)\n",
    "messages.columns = ['target','msg']\n",
    "### For verifying answer:\n",
    "display(messages.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 04",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Assignment-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "As usual in supervised learning, you want to split the data into training and testing data sets so that the model can be assessed after fitting it to the data. You'll use the `train_test_split` function from the Scikit-Learn module `sklearn.model_selelction` that does this work for you. Notice the use of the keyword argument `stratify` to ensure that the proportion of ham and spam messages match in the training and the testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4179 training observations & 1393 testing observations.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "messages_train, messages_test = train_test_split(messages, random_state=13, stratify=messages['target'])\n",
    "print('There are {} training observations & {} testing observations.\\n'\n",
    "        .format(len(messages_train), len(messages_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Assignment-Contents)\n",
    "<a id=\"q-priors\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "### Question 5: Getting priors\n",
    "\n",
    "Estimate prior probabilities of messages being `ham` and `spam` empirically using the training set.\n",
    "\n",
    "+ Construct a Pandas Series `priors` with Index values `'ham'` and `'spam'` and corresponding values given by the fraction of `ham` and `spam` messages (respectively) in the training set `messages_train`.\n",
    "+ HINT: the Pandas Series method `value_counts` is useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training priors (%):\n",
      "===================\n",
      "ham     86.599665\n",
      "spam    13.400335\n",
      "Name: target, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "### QUESTION 5:\n",
    "### Assign a Pandas Series to priors as described above.\n",
    "### YOUR SOLUTION HERE:\n",
    "priors = messages_train['target'].value_counts(normalize=True)\n",
    "### For verifying answer:\n",
    "print('Training priors (%):\\n===================\\n{}\\n'.format(100 * priors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 05",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Assignment-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Processing SMS messages\n",
    "\n",
    "To get a sense of how to build the Naive Bayes classifier, you can examine a few of the spam messages in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3862</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free Msg: Ringtone!From: http://tms. widelive....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3421</th>\n",
       "      <td>spam</td>\n",
       "      <td>As a valued customer, I am pleased to advise y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5164</th>\n",
       "      <td>spam</td>\n",
       "      <td>Congrats 2 mobile 3G Videophones R yours. call...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>spam</td>\n",
       "      <td>Customer service annoncement. You have a New Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3571</th>\n",
       "      <td>spam</td>\n",
       "      <td>Customer Loyalty Offer:The NEW Nokia6650 Mobil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target                                                msg\n",
       "3862   spam  Free Msg: Ringtone!From: http://tms. widelive....\n",
       "3421   spam  As a valued customer, I am pleased to advise y...\n",
       "5164   spam  Congrats 2 mobile 3G Videophones R yours. call...\n",
       "159    spam  Customer service annoncement. You have a New Y...\n",
       "3571   spam  Customer Loyalty Offer:The NEW Nokia6650 Mobil..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "is_spam = (messages_train['target']=='spam')\n",
    "display(messages_train.loc[is_spam].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of these messages flagged as spam include the word *\"customer\"*. Notice that the word is sometimes capitalized, but not always. To catch this, you can write a boolean-valued utility function `has_word` that signals the presence of a word as a substring of a message in a case-insensitive way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def has_word(msg, word):\n",
    "    '''Returns True if string word is contained within string msg\n",
    "    Ignores case, i.e., matches on lower or upper case match.'''\n",
    "    return (word.lower() in msg.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is *extremely* simplified. That is, in practical filters, the message would not only be converted to lower case. It would be split into individual words, punctuation & stop words would be removed, words would likely be *stemmed* or lemmatized (to ensure, for example, matches on *organize*, *organizes*, *organized*, and *organizing* will all be found) and so on. For this assignment, you will use this very crude form of matching to avoid some of those complications. You'll find that even with this crude model, spam and ham can be flagged reasonably well.\n",
    "\n",
    "Let's extract one of the messages found above and apply `has_word` to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True \n",
      "\n",
      " Customer service annoncement. You have a New Years delivery waiting for you. Please call 07046744435 now to arrange delivery\n"
     ]
    }
   ],
   "source": [
    "k, word = 159, 'customer'\n",
    "msg = messages_train.loc[k, 'msg']\n",
    "# Verifying that has_word works as intended.\n",
    "print(has_word(msg, word), '\\n\\n', msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Assignment-Contents)\n",
    "<a id=\"q-likelihoods\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "### Question 6: Computing likelihoods\n",
    "\n",
    "Your task now is to encapsulate the logic above into another function that operates on DataFrames. That is, complete the function `get_likelihoods` that computes the *empirical likelihoods* of a word being in a message given all possible categories (`ham` and `spam` in this case).\n",
    "+ The inputs are `word` (a string) and `df` (a DataFrame). The DataFrame is assumed to have columns `'target'` and `'msg'` (the first of which is assumed to be categorical, the second of which is assumed to contain strings (messages)). This is precisely the form of `messages_train`. \n",
    "+ The function returns a Pandas Series whose Index contains the categories of `df['target']` (`ham` and `spam` in this case). The values of the Series are the fractions of rows in each category in which the column `'msg'` contains `word` as a substring (case-insensitive).\n",
    "+ Do *not* assume Laplace smoothing here, i.e., likelihoods of zero are possible if a given word does not appear in the message DataFrame's `'msg'` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer:\n",
      "=========\n",
      "ham     0.002211\n",
      "spam    0.067857\n",
      "dtype: float64 \n",
      "\n",
      "congrats:\n",
      "=========\n",
      "ham     0.001934\n",
      "spam    0.016071\n",
      "dtype: float64 \n",
      "\n",
      "meeting:\n",
      "========\n",
      "ham     0.008842\n",
      "spam    0.000000\n",
      "dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "### QUESTION 6:\n",
    "### Complete the function get_likelihoods as described above.\n",
    "###\n",
    "def get_likelihoods(word, df):\n",
    "    '''Computes empirical fractions of rows of *df* that have *word* in 'msg' column.\n",
    "    INPUT:\n",
    "      word:        String to match (case-insensitive)\n",
    "      df:          DataFrame with columns 'target' (categorical) and 'msg' (string objects)\n",
    "    OUTPUT:\n",
    "      likelihoods: Series indexed by categories of df['target'] with empirical fraction of\n",
    "                   rows in which df['msg'] includes word as a substring (case-insensitive).\n",
    "                   Do *not* employ Laplace smoothing here.\n",
    "    EXAMPLE:\n",
    "    >>> l = get_likelihoods('customer', messages_train)\n",
    "    >>> print(l)\n",
    "        ham     0.002211\n",
    "        spam    0.067857\n",
    "        dtype: float64\n",
    "    >>> l = get_likelihoods('congrats', messages_train)\n",
    "    >>> print(l)\n",
    "        ham     0.001934\n",
    "        spam    0.016071\n",
    "        dtype: float64 \n",
    "    '''\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n",
    "    filt = lambda m: has_word(m,word)\n",
    "    fractions = {}\n",
    "    for category in np.unique(df.target):\n",
    "        df_new = df.loc[df['target'] == category]\n",
    "        counts = df_new['msg'].map(filt).sum()\n",
    "        fractions[category] = counts / len(df_new)\n",
    "        \n",
    "    likelihoods = pd.Series(fractions)\n",
    "    return likelihoods\n",
    "        \n",
    "\n",
    "### For verifying answer:\n",
    "for word in ['customer', 'congrats', 'meeting']:\n",
    "    print('{}:\\n={}'.format(word,'='*len(word)))\n",
    "    print(get_likelihoods(word, messages_train), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 06",
     "locked": true,
     "points": "15",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Assignment-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Building the Naive Bayes model\n",
    "\n",
    "You may have noticed in Question 6 an unusual result with the input word \"meeting\":\n",
    "\n",
    "```\n",
    ">>> get_likelihoods('meeting', messages_train)\n",
    "meeting:\n",
    "========\n",
    "ham     0.008842\n",
    "spam    0.000000\n",
    "dtype: float64 \n",
    "\n",
    "```\n",
    "That is, the empirical likelihood of finding the word \"meeting\" in a spam message is zero. This is an artifact of the particular corpus of training messages; that is, that word does not happen to occur in any of the finite number of spam messages in the training set. It is generally not reasonable to assume that, more generally, the likelihood of the word \"meeting\" occurring in *any* incoming spam message is zero.\n",
    "\n",
    "To compensate for this kind of problem, you can use [*Laplace smoothing*](https://en.wikipedia.org/wiki/Laplacian_smoothing). That is, modify the computation of an empirical likelihood as follows:\n",
    "\n",
    "$$ p(w\\,|\\,C) \\simeq \\frac{|C \\cap w| + \\color{red}{\\beta}}{n_{C} + \\color{red}{n_{\\text{w}}} } $$\n",
    "\n",
    "In the above, $C$ refers to any available categories in the classification problem (`ham` or `spam` here), $w$ is the sought event (in this case, a word $w$ occurring in a messages in the set $C$), $\\beta$ is a *smoothing parameter* (typically 1) and $n_w$ is another parameter (in this case, the number of words in the vocabulary to scan for). This has the effect of augmenting likelihoods away from zero (that are problematic when multiplying likelihoods together as is required for naive Bayes classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Assignment-Contents)\n",
    "<a id=\"q-smoothed\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "### Question 7: Computing smoothed likelihoods\n",
    "\n",
    "Your task now is to generalize the function `get_likelihoods` from Question 6 to yield a function `get_smoothed_likelihoods`. \n",
    "+ The inputs are almost the same as in Question 6. The first input is a list `words` consisting of the vocabulary of words to look for in messages. The second input `df` is a DataFrame with the same requirements as before. There is also an extra input `beta` (whose default value is one) that is used to compute the smoothed likelihoods.\n",
    "+ The output is a Pandas DataFrame whose Index contains the categories of `df['target']` (`ham` and `spam` in this case) and whose columns are the words in the input `words`. The corresponding values of the DataFrame are the fractions of rows in each category in which the column `'msg'` contains the corresponding word as a substring (case-insensitive).\n",
    "+ In this case, *apply Laplace smoothing* i.e., likelihoods of zero are not possible even if a given word does not appear in the message DataFrame's `'msg'` column.\n",
    "+ *NOTE*: Although the construction demonstrated in the lecture video 19-4 is useful for these exercises, be aware that the DataFrame of likelihoods obtained in the video is *transposed* relative to the requirements here. That is, the comparable DataFrame computed in the lecture video has the categories `spam` and `ham` as *column* index labels and the list of words as *row* index labels. Again, this is the *opposite* of what is required here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      customer  congrats   meeting\n",
      "ham   0.002485  0.002209  0.009111\n",
      "spam  0.069272  0.017762  0.001776 \n",
      "\n",
      "      customer  congrats   meeting\n",
      "ham   0.002209  0.001933  0.008835\n",
      "spam  0.067496  0.015986  0.000000\n"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "### QUESTION 7:\n",
    "### Complete the function get_smoothed_likelihoods as described above.\n",
    "###\n",
    "def get_smoothed_likelihoods(words, df, beta=1):\n",
    "    '''Computes empirical fractions of rows of *df* that have *word* in 'msg' column.\n",
    "    INPUT:\n",
    "      words:       List of strings to match (case-insensitive)\n",
    "      df:          DataFrame with columns 'target' (categorical) and 'msg' (string objects)\n",
    "      beta:        (default value 1) Smoothing constant to add to numerator\n",
    "    OUTPUT:\n",
    "      likelihoods: DataFrame with categories of df['target'] (row) Index and words as column\n",
    "                   index. The entries are the empirical fractions of rows in which df['msg'] includes\n",
    "                   each word as a substring (case-insensitive).\n",
    "                   Empirical fractions computed using Laplace smoothing here.\n",
    "    EXAMPLE:\n",
    "    >>> words = ['customer', 'congrats', 'meeting']\n",
    "    >>> likelihoods = get_smoothed_likelihoods(words, messages_train)\n",
    "    >>> print(likelihoods)\n",
    "              customer  congrats   meeting\n",
    "        ham   0.002485  0.002209  0.009111\n",
    "        spam  0.069272  0.017762  0.001776\n",
    "    >>> likelihoods = get_smoothed_likelihoods(words, messages_train, beta=0)\n",
    "    >>> print(likelihoods) # without smoothing (mostly)\n",
    "              customer  congrats   meeting\n",
    "        ham   0.002209  0.001933  0.008835\n",
    "        spam  0.067496  0.015986  0.000000\n",
    "    '''\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n",
    "    n_words = len(words)\n",
    "    data = {}\n",
    "    for word in words:\n",
    "        filt = lambda m: has_word(m, word)\n",
    "        fractions = {}\n",
    "        for category in np.unique(df.target):\n",
    "            df_new = df.loc[df['target'] == category]\n",
    "            counts = df_new['msg'].map(filt).sum()\n",
    "            fractions[category] = (counts + beta) / (len(df_new) + n_words)\n",
    "        data[word] = pd.Series(fractions)\n",
    "    likelihoods = pd.DataFrame(data=data)\n",
    "    return likelihoods\n",
    "    \n",
    "### For verifying answer:\n",
    "words = ['customer', 'congrats', 'meeting']\n",
    "print(get_smoothed_likelihoods(words, messages_train), '\\n')\n",
    "print(get_smoothed_likelihoods(words, messages_train, beta=0)) # without smoothing (mostly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 07",
     "locked": true,
     "points": "15",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Assignment-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Putting it all together\n",
    "\n",
    "Finally, you can now try to see how to put these pieces together to classify the testing data as spam or ham. Unfortunately, given the oversimplifications in the feature extraction used, it's more instructive to consider only a small vocabulary of words and a selected subset of testing messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# this is a highly restricted vocabulary of words to help distinguish spam & ham\n",
    "vocab = ['winner', 'congratulations', 'free', 'contact', 'holiday', 'price',\n",
    "         'urgent', 'cost', 'credit', 'won', 'new', 'work', 'loan', 'return',\n",
    "         'insurance', 'bank', 'sale', 'safe', 'red', 'alright', 'place',\n",
    "         'house', 'buy', 'hello', 'hi', 'got', 'have', 'well', 'vacation',\n",
    "         'thing', 'cat', 'car', 'kitchen', 'want', 'waiting', 'dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3420</th>\n",
       "      <td>spam</td>\n",
       "      <td>Do you want a new Video phone? 600 anytime any...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>ham</td>\n",
       "      <td>sorry, no, have got few things to do. may be i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target                                                msg\n",
       "3420   spam  Do you want a new Video phone? 600 anytime any...\n",
       "624     ham  sorry, no, have got few things to do. may be i..."
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a restricted set of test messages to examine\n",
    "selected = [3420, 624]\n",
    "test_set = messages_test.loc[selected]\n",
    "test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the first message, see which words from our vocabulary it contains and then compute the smoothed likelihoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "idx = 3420\n",
    "test_msg = test_set.loc[idx, 'msg']\n",
    "y_true = test_set.loc[idx,'target']\n",
    "words = [word for word in vocab if has_word(test_msg, word)]\n",
    "likelihoods = get_smoothed_likelihoods(words, messages_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key assumption in [*naive Bayes classification*](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) is that the *likelihoods are independent* (i.e., that the likelihood of each word occurring in a message known to be spam or ham is independent of the other words occurring). This is the \"naive\" assumption, but it makes the computations much simpler (particularly when the vocabulary is large, say, thousands of words).\n",
    "\n",
    "Under this assumption, the joint likelihood of the words $w_1$ and $w_2$ occuring in category $C$ can be determined as a product:\n",
    " $$ \\boxed{p(w_1 \\cap w_2 \\,|\\,C) = p(w_1\\,|\\,C)\\times p(w_2\\,|\\,C)}. $$\n",
    " This generalizes easily to arbitrarily many words/features (and even arbitrarily many classes in a multi-class classification problem): \n",
    "  $$ \\boxed{p(w_1, w_2, \\dotsc, w_d\\,|\\, C) = \\prod_{k=1}^{d}p(w_{k}\\,|\\,C)}.$$\n",
    "\n",
    "\n",
    "In the implementation developed so far, this computation can be carried out easily using the Pandas DataFrame method `prod`. Once the joint likelihoods are known, they can be combined with the priors to compute the posterior probabilities as Bayes' theorem tells us. And when the posterior probabilities are known for both the ham and spam classes, the larger of the two can be used to decide how to classify a new message instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posteriors:\n",
      "ham     0.036228\n",
      "spam    0.963772\n",
      "dtype: float64\n",
      "\n",
      "Do you want a new Video phone? 600 anytime any network mins 400 Inclusive Video calls AND downloads 5 per week Free delTOMORROW call 08002888812 or reply NOW\n",
      "\n",
      "y_true = spam\n",
      "y_pred = spam\n"
     ]
    }
   ],
   "source": [
    "joint_likelihoods = likelihoods.prod(axis=1)    # Use assumption of independence\n",
    "evidence = (joint_likelihoods * priors).sum()\n",
    "posteriors = joint_likelihoods * priors / evidence  # Bayes' theorem\n",
    "print('Posteriors:\\n{}\\n'.format(posteriors))\n",
    "y_pred = posteriors.idxmax()     # Take largest posterior probability to predict class\n",
    "print('{}\\n\\ny_true = {}\\ny_pred = {}'.format(test_msg, y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat the preceding computation on the other test message. First, let's extract the words from `vocab` that actually occur in this message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = 624\n",
    "test_msg = test_set.loc[idx, 'msg']\n",
    "y_true = test_set.loc[idx,'target']\n",
    "words = [word for word in vocab if has_word(test_msg, word)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Next, let's compute the likelihoods and the evidence (remember, you computed the priors empirically from the training data in Question 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "likelihoods = get_smoothed_likelihoods(words, messages_train)\n",
    "joint_likelihoods = likelihoods.prod(axis=1)    # Use assumption of independence\n",
    "evidence = (joint_likelihoods * priors).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We now have the pieces in place to apply Bayes' theorem to compute the posteriors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posteriors:\n",
      "ham     0.98111\n",
      "spam    0.01889\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "posteriors = joint_likelihoods * priors / evidence  # Bayes' theorem\n",
    "print('Posteriors:\\n{}\\n'.format(posteriors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Finally, the posterior probabilities for each class can be used to make a decision about classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorry, no, have got few things to do. may be in pub later.\n",
      "\n",
      "y_true = ham\n",
      "y_pred = ham\n"
     ]
    }
   ],
   "source": [
    "y_pred = posteriors.idxmax()     # Take largest posterior probability to predict class\n",
    "print('{}\\n\\ny_true = {}\\ny_pred = {}'.format(test_msg, y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "For the last question, you will use a larger (carefully chosen) subset of the test data to see if you can use this selection of words to classify ham and spam messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected = [1303, 2124, 4073, 4967, 2686, 1944, 4923, 1895, 2876, 3455, 113, 4729, 2946,\n",
    "            4801, 4009, 838, 3509, 3675, 3595, 5535, 4918, 4935, 1936, 1491, 3772, 4905,\n",
    "            3789, 901, 3164, 5566, 5482, 4133, 4543, 2422, 3456, 2849, 4497, 2514]\n",
    "test_set = messages_test.loc[selected]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Assignment-Contents)\n",
    "<a id=\"q-predict\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "### Question 8: Predicting spam\n",
    "\n",
    "Your final task is to complete a function `predict_nb` that implements the computations just applied to the preceding two messages.\n",
    "+ The mandatory input is `test_msg` (a *single message*, i.e., a Python string).\n",
    "+ As optional keyword arguments, `predict_nb` accepts `word_list` (with default value `vocab` as provided above) and `data` (with default value `messages_train` as computed earlier).\n",
    "+ The value returned is a category from `data['target']` (`ham` or `spam` in this case).\n",
    "+ The function should be ready to map onto a Series as shown below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>pred</th>\n",
       "      <th>msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1303</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>FRAN I DECIDED 2 GO N E WAY IM COMPLETELY BROK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2124</th>\n",
       "      <td>spam</td>\n",
       "      <td>spam</td>\n",
       "      <td>+123 Congratulations - in this week's competit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4073</th>\n",
       "      <td>spam</td>\n",
       "      <td>spam</td>\n",
       "      <td>Loans for any purpose even if you have Bad Cre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4967</th>\n",
       "      <td>spam</td>\n",
       "      <td>spam</td>\n",
       "      <td>URGENT! We are trying to contact U. Todays dra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2686</th>\n",
       "      <td>spam</td>\n",
       "      <td>spam</td>\n",
       "      <td>URGENT! We are trying to contact U. Todays dra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1944</th>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>I got lousy sleep. I kept waking up every 2 ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4923</th>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>Hi Dear Call me its urgnt. I don't know whats ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1895</th>\n",
       "      <td>spam</td>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey U, i just got 1 of these video/pic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2876</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>Idk. You keep saying that you're not, but sinc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3455</th>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>I dont have any of your file in my bag..i was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>I'm ok wif it cos i like 2 try new things. But...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4729</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>I (Career Tel) have added u as a contact on IN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2946</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>Hey babe, sorry i didn't get sooner. Gary can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4801</th>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>its cool but tyler had to take off so we're go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4009</th>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>Forgot you were working today! Wanna chat, but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>Sir, I have been late in paying rent for the p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3509</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>Camera quite good, 10.1mega pixels, 3optical a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3675</th>\n",
       "      <td>spam</td>\n",
       "      <td>spam</td>\n",
       "      <td>You have won a Nokia 7250i. This is what you g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595</th>\n",
       "      <td>spam</td>\n",
       "      <td>spam</td>\n",
       "      <td>Do you want a New Nokia 3510i Colour Phone Del...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5535</th>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>I know you are thinkin malaria. But relax, chi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4918</th>\n",
       "      <td>spam</td>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4935</th>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>Hey do you want anything to buy:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1936</th>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>Did either of you have any idea's? Do you know...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1491</th>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>Cant believe i said so many things to you this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3772</th>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>Hi, wlcome back, did wonder if you got eaten b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4905</th>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>no, i *didn't* mean to post it. I wrote it, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3789</th>\n",
       "      <td>spam</td>\n",
       "      <td>spam</td>\n",
       "      <td>Want to funk up ur fone with a weekly new tone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>Probably money worries. Things are coming due ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3164</th>\n",
       "      <td>spam</td>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried to contact ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>spam</td>\n",
       "      <td>spam</td>\n",
       "      <td>REMINDER FROM O2: To get 2.50 pounds free call...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5482</th>\n",
       "      <td>spam</td>\n",
       "      <td>spam</td>\n",
       "      <td>URGENT We are trying to contact you Last weeke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4133</th>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>Hi baby ive just got back from work and i was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4543</th>\n",
       "      <td>spam</td>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hi baby wow just got a new cam moby. W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2422</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>Err... Cud do. I'm going to  at 8pm. I haven't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3456</th>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>No need lar. Jus testing e phone card. Dunno n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2849</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>Sad story of a Man - Last week was my b'day. M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4497</th>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>In case you wake up wondering where I am, I fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2514</th>\n",
       "      <td>spam</td>\n",
       "      <td>spam</td>\n",
       "      <td>U have won a nokia 6230 plus a free digital ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target  pred                                                msg\n",
       "1303    ham  spam  FRAN I DECIDED 2 GO N E WAY IM COMPLETELY BROK...\n",
       "2124   spam  spam  +123 Congratulations - in this week's competit...\n",
       "4073   spam  spam  Loans for any purpose even if you have Bad Cre...\n",
       "4967   spam  spam  URGENT! We are trying to contact U. Todays dra...\n",
       "2686   spam  spam  URGENT! We are trying to contact U. Todays dra...\n",
       "1944    ham   ham  I got lousy sleep. I kept waking up every 2 ho...\n",
       "4923    ham   ham  Hi Dear Call me its urgnt. I don't know whats ...\n",
       "1895   spam  spam  FreeMsg Hey U, i just got 1 of these video/pic...\n",
       "2876    ham  spam  Idk. You keep saying that you're not, but sinc...\n",
       "3455    ham   ham  I dont have any of your file in my bag..i was ...\n",
       "113     ham   ham  I'm ok wif it cos i like 2 try new things. But...\n",
       "4729    ham  spam  I (Career Tel) have added u as a contact on IN...\n",
       "2946    ham  spam  Hey babe, sorry i didn't get sooner. Gary can ...\n",
       "4801    ham   ham  its cool but tyler had to take off so we're go...\n",
       "4009    ham   ham  Forgot you were working today! Wanna chat, but...\n",
       "838     ham  spam  Sir, I have been late in paying rent for the p...\n",
       "3509    ham  spam  Camera quite good, 10.1mega pixels, 3optical a...\n",
       "3675   spam  spam  You have won a Nokia 7250i. This is what you g...\n",
       "3595   spam  spam  Do you want a New Nokia 3510i Colour Phone Del...\n",
       "5535    ham   ham  I know you are thinkin malaria. But relax, chi...\n",
       "4918   spam  spam  This is the 2nd time we have tried 2 contact u...\n",
       "4935    ham   ham                  Hey do you want anything to buy:)\n",
       "1936    ham   ham  Did either of you have any idea's? Do you know...\n",
       "1491    ham   ham  Cant believe i said so many things to you this...\n",
       "3772    ham   ham  Hi, wlcome back, did wonder if you got eaten b...\n",
       "4905    ham   ham  no, i *didn't* mean to post it. I wrote it, an...\n",
       "3789   spam  spam  Want to funk up ur fone with a weekly new tone...\n",
       "901     ham   ham  Probably money worries. Things are coming due ...\n",
       "3164   spam  spam  This is the 2nd time we have tried to contact ...\n",
       "5566   spam  spam  REMINDER FROM O2: To get 2.50 pounds free call...\n",
       "5482   spam  spam  URGENT We are trying to contact you Last weeke...\n",
       "4133    ham   ham  Hi baby ive just got back from work and i was ...\n",
       "4543   spam  spam  FreeMsg Hi baby wow just got a new cam moby. W...\n",
       "2422    ham  spam  Err... Cud do. I'm going to  at 8pm. I haven't...\n",
       "3456    ham   ham  No need lar. Jus testing e phone card. Dunno n...\n",
       "2849    ham  spam  Sad story of a Man - Last week was my b'day. M...\n",
       "4497    ham   ham  In case you wake up wondering where I am, I fo...\n",
       "2514   spam  spam  U have won a nokia 6230 plus a free digital ca..."
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### GRADED\n",
    "### QUESTION 8:\n",
    "### Complete the function predict_nb as described above\n",
    "###\n",
    "def predict_nb(test_msg, word_list=vocab, data=messages_train):\n",
    "    '''Returns a category from data.target according to how test_msg classifies\n",
    "    INPUT:\n",
    "      test_msg:    String (message)\n",
    "      word_list:   List of strings to match (case-insensitive); default value vocab\n",
    "      data:        DataFrame with columns 'target' (categorical) and 'msg' (string objects);\n",
    "                   default value messages_train\n",
    "    OUTPUT:\n",
    "      y_pred:      category from data.target\n",
    "    EXAMPLE:\n",
    "    >>> msg = messages_test.loc[624, 'msg']\n",
    "    >>> print(predict_nb(msg))\n",
    "        ham\n",
    "    >>> msg = messages_test.loc[3420, 'msg']\n",
    "    >>> print(predict_nb(msg))\n",
    "        spam\n",
    "    '''\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n",
    "    words = [word for word in word_list if has_word(test_msg,word)]\n",
    "    likelihoods = get_smoothed_likelihoods(words, data)\n",
    "    joint_likelihoods = likelihoods.prod(axis=1)\n",
    "    evidence = (joint_likelihoods*priors).sum()\n",
    "    posteriors = (joint_likelihoods*priors) / evidence\n",
    "    y_pred = posteriors.idxmax()\n",
    "    return y_pred\n",
    "\n",
    "### For verifying answer:\n",
    "test_set['pred'] = test_set['msg'].map(predict_nb)\n",
    "result = test_set[['target', 'pred', 'msg']]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 08",
     "locked": true,
     "points": "15",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Assignment-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "As you can see, roughly 40 test messages are predicted with about 79% accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(result['target'] == result['pred']).sum() / len(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Of course, the test examples were selected to maximize overlap with the selected vocabulary. Nevertheless, this very simple implementation can be extended readily with more realistic feature extraction and a broader vocabulary. Scikit-Learn's `naive_bayes` and `feature_extraction` modules provide tools for building more useful models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Assignment-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "+ [*Naive Bayes Classifier* ](https://en.wikipedia.org/wiki/Naive_bayes) (Wikipedia)\n",
    "+ [Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html) (Scikit-Learn documentation)\n",
    "+ [*Python Data Science Handbook*](https://jakevdp.github.io/PythonDataScienceHandbook/) by Jake Vanderplas\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": [],
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": [],
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
